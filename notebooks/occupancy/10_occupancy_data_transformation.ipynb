{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Occupancy Data Transformation\n",
    "\n",
    "This notebook transforms individual parking session data into hourly occupancy predictions \n",
    "\n",
    "## The Shift in Approach\n",
    "\n",
    "**Previous approach (notebooks 10-11):** Predict individual session durations\n",
    "- Target: `duration_hours` per parking session\n",
    "\n",
    "\n",
    "**New approach (this notebook):** Predict hourly occupancy rates\n",
    "- Target: `occupancy_count` or `occupancy_rate` per zone per hour\n",
    "- More predictable: Aggregate patterns smooth out individual variance\n",
    "- Directly useful: \"CUE Garage will be 85% full at 10am Monday\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load AMP Data\n",
    "\n",
    "Load the base preprocessed AMP parking session data (without enrichment).\n",
    "Calendar and weather features will be added to the occupancy data later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AMP preprocessed CLEANED data (excludes B St, JumpTest, non-existent lots)\n",
    "# This is the cleaned version from notebook 14\n",
    "amp = pd.read_csv('../../data/processed/amp_preprocessed_clean.csv', parse_dates=['Start_Date', 'End_Date'])\n",
    "\n",
    "print(f\"Total parking sessions: {len(amp):,}\")\n",
    "print(f\"Date range: {amp['Start_Date'].min()} to {amp['Start_Date'].max()}\")\n",
    "print(f\"Unique zones: {amp['Zone'].nunique()}\")\n",
    "print(f\"\\nZones: {sorted(amp['Zone'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 15-Minute Time Grid\n",
    "\n",
    "Generate all possible (Date, Hour, Minute) combinations for the date range.\n",
    "Each day has 96 intervals (24 hours × 4 intervals per hour)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get date range\n",
    "start_date = amp['Start_Date'].min().date()\n",
    "end_date = amp['Start_Date'].max().date()\n",
    "\n",
    "# Create date range\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Create hour range (0-23) and minute intervals (0, 15, 30, 45)\n",
    "hours = range(24)\n",
    "minutes = [0, 15, 30, 45]\n",
    "\n",
    "# Create all combinations of date, hour, and minute (15-min intervals)\n",
    "time_grid = pd.MultiIndex.from_product(\n",
    "    [date_range, hours, minutes],\n",
    "    names=['date', 'hour', 'minute']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# Create datetime column for easier processing\n",
    "time_grid['datetime'] = (\n",
    "    pd.to_datetime(time_grid['date']) + \n",
    "    pd.to_timedelta(time_grid['hour'], unit='h') +\n",
    "    pd.to_timedelta(time_grid['minute'], unit='m')\n",
    ")\n",
    "\n",
    "print(f\"Time grid created: {len(time_grid):,} 15-minute intervals\")\n",
    "print(f\"Intervals per day: {len(hours) * len(minutes)}\")\n",
    "print(f\"Date range: {time_grid['date'].min()} to {time_grid['date'].max()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(time_grid.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Occupancy for Each 15-Minute Interval\n",
    "\n",
    "For each zone and 15-minute interval, count how many parking sessions were active during that interval.\n",
    "\n",
    "A session is \"active\" during an interval if:\n",
    "- Start_Date <= interval_end AND End_Date >= interval_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique zones\n",
    "zones = amp['Zone'].unique()\n",
    "\n",
    "print(f\"Processing occupancy for {len(zones)} zones...\")\n",
    "print(\"This will take 20-40 minutes for 15-minute intervals...\")\n",
    "print(f\"Processing {len(zones) * len(date_range) * 24 * 4:,} zone-interval combinations\")\n",
    "\n",
    "# Create zone-interval grid (15-minute intervals)\n",
    "zone_interval_grid = pd.MultiIndex.from_product(\n",
    "    [zones, date_range, hours, minutes],\n",
    "    names=['Zone', 'date', 'hour', 'minute']\n",
    ").to_frame(index=False)\n",
    "\n",
    "# Create datetime columns for interval start and end\n",
    "zone_interval_grid['interval_start'] = (\n",
    "    pd.to_datetime(zone_interval_grid['date']) + \n",
    "    pd.to_timedelta(zone_interval_grid['hour'], unit='h') +\n",
    "    pd.to_timedelta(zone_interval_grid['minute'], unit='m')\n",
    ")\n",
    "zone_interval_grid['interval_end'] = zone_interval_grid['interval_start'] + pd.Timedelta(minutes=15)\n",
    "\n",
    "print(f\"Zone-interval grid created: {len(zone_interval_grid):,} combinations\")\n",
    "print(f\"\\nCalculating occupancy counts for 15-minute intervals...\")\n",
    "print(f\"Sample intervals:\")\n",
    "print(zone_interval_grid[['Zone', 'interval_start', 'interval_end']].head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate occupancy using a vectorized approach\n",
    "# For each zone-interval, count sessions that overlap with that 15-minute interval\n",
    "\n",
    "occupancy_list = []\n",
    "zone_count=0\n",
    "for zone in zones:\n",
    "    zone_count += 1\n",
    "    print(f\"Processing zone: {zone}: {round(zone_count/len(zones),2)} ({zone_count/len(zones)*100:.1f}%)\")\n",
    "    \n",
    "    # Filter sessions for this zone\n",
    "    zone_sessions = amp[amp['Zone'] == zone].copy()\n",
    "    \n",
    "    # Get time grid for this zone\n",
    "    zone_grid = zone_interval_grid[zone_interval_grid['Zone'] == zone].copy()\n",
    "    \n",
    "    # For each 15-minute interval in the grid, count overlapping sessions\n",
    "    for idx, row in zone_grid.iterrows():\n",
    "        interval_start = row['interval_start']\n",
    "        interval_end = row['interval_end']\n",
    "        \n",
    "        # Count sessions active during this 15-minute interval\n",
    "        # Session overlaps if: session_start < interval_end AND session_end > interval_start\n",
    "        active_sessions = zone_sessions[\n",
    "            (zone_sessions['Start_Date'] < interval_end) & \n",
    "            (zone_sessions['End_Date'] > interval_start)\n",
    "        ]\n",
    "        \n",
    "        occupancy_list.append({\n",
    "            'Zone': zone,\n",
    "            'date': row['date'],\n",
    "            'hour': row['hour'],\n",
    "            'minute': row['minute'],\n",
    "            'datetime': interval_start,\n",
    "            'occupancy_count': len(active_sessions)\n",
    "        })\n",
    "\n",
    "# Convert to DataFrame\n",
    "occupancy_df = pd.DataFrame(occupancy_list)\n",
    "\n",
    "print(f\"\\nOccupancy data created: {len(occupancy_df):,} zone-interval records (15-min granularity)\")\n",
    "print(f\"Date range: {occupancy_df['date'].min()} to {occupancy_df['date'].max()}\")\n",
    "print(f\"Sample data:\")\n",
    "print(occupancy_df.head(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract temporal features\n",
    "occupancy_df['year'] = occupancy_df['datetime'].dt.year\n",
    "occupancy_df['month'] = occupancy_df['datetime'].dt.month\n",
    "occupancy_df['day'] = occupancy_df['datetime'].dt.day\n",
    "occupancy_df['day_of_week'] = occupancy_df['datetime'].dt.dayofweek\n",
    "occupancy_df['day_name'] = occupancy_df['datetime'].dt.day_name()\n",
    "occupancy_df['is_weekend'] = (occupancy_df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "# Add minute_interval as a feature (0, 15, 30, 45)\n",
    "occupancy_df['minute_interval'] = occupancy_df['minute']\n",
    "\n",
    "# Time of day categories\n",
    "def categorize_time_of_day(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Late Night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "occupancy_df['time_of_day'] = occupancy_df['hour'].apply(categorize_time_of_day)\n",
    "\n",
    "print(\"Temporal features added (including 15-minute intervals)\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(occupancy_df[['Zone', 'datetime', 'hour', 'minute', 'occupancy_count']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Calendar Events\n",
    "\n",
    "Add game days, finals weeks, and breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calendar data\n",
    "games = pd.read_csv('../../data/football_games.csv')\n",
    "calendar = pd.read_csv('../../data/academic_calendar.csv')\n",
    "\n",
    "# Parse dates\n",
    "games['Date'] = pd.to_datetime(games['Date'])\n",
    "calendar['Start_Date'] = pd.to_datetime(calendar['Start_Date'])\n",
    "calendar['End_Date'] = pd.to_datetime(calendar['End_Date'])\n",
    "\n",
    "# Convert occupancy_df['date'] to datetime for comparison\n",
    "occupancy_df['date'] = pd.to_datetime(occupancy_df['date'])\n",
    "\n",
    "# Mark game days\n",
    "game_dates = games['Date'].dt.date.unique()\n",
    "occupancy_df['is_game_day'] = occupancy_df['date'].dt.date.isin(game_dates).astype(int)\n",
    "\n",
    "# Mark calendar events\n",
    "for event_type in ['Dead_Week', 'Finals_Week', 'Spring_Break', 'Thanksgiving_Break', 'Winter_Break', 'University_Holiday']:\n",
    "    event_periods = calendar[calendar['Event_Type'] == event_type]\n",
    "    \n",
    "    occupancy_df[f'is_{event_type.lower()}'] = 0\n",
    "    \n",
    "    for _, period in event_periods.iterrows():\n",
    "        # Compare datetime columns directly\n",
    "        mask = (occupancy_df['date'] >= period['Start_Date']) & \\\n",
    "               (occupancy_df['date'] <= period['End_Date'])\n",
    "        occupancy_df.loc[mask, f'is_{event_type.lower()}'] = 1\n",
    "\n",
    "# Create combined break indicator\n",
    "occupancy_df['is_any_break'] = (\n",
    "    occupancy_df['is_spring_break'] | \n",
    "    occupancy_df['is_thanksgiving_break'] | \n",
    "    occupancy_df['is_winter_break']\n",
    ").astype(int)\n",
    "\n",
    "print(\"Calendar events merged\")\n",
    "print(f\"\\nGame days: {occupancy_df['is_game_day'].sum():,}\")\n",
    "print(f\"Dead weeks: {occupancy_df['is_dead_week'].sum():,}\")\n",
    "print(f\"Finals weeks: {occupancy_df['is_finals_week'].sum():,}\")\n",
    "print(f\"Break periods: {occupancy_df['is_any_break'].sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load weather data\n",
    "weather = pd.read_csv('../../data/weather_pullman_2020_2025.csv')\n",
    "weather['date'] = pd.to_datetime(weather['date']).dt.date\n",
    "\n",
    "# Convert occupancy_df['date'] to date object for merging with weather\n",
    "occupancy_df['date_for_merge'] = occupancy_df['date'].dt.date\n",
    "\n",
    "# Merge weather\n",
    "occupancy_df = occupancy_df.merge(weather, left_on='date_for_merge', right_on='date', how='left')\n",
    "\n",
    "# Drop the temporary merge column and the redundant date column from weather\n",
    "occupancy_df = occupancy_df.drop(columns=['date_for_merge', 'date_y'])\n",
    "occupancy_df = occupancy_df.rename(columns={'date_x': 'date'})\n",
    "\n",
    "print(\"Weather data merged\")\n",
    "print(f\"\\nWeather columns added: {[col for col in weather.columns if col != 'date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Camera Classification\n",
    "\n",
    "Add fixed camera features from notebook 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load camera classification\n",
    "try:\n",
    "    camera_features = pd.read_csv('../../data/processed/fixed_camera_classification.csv')\n",
    "    print(f\"Camera classification loaded: {len(camera_features)} lots\")\n",
    "    \n",
    "    # Convert Lot_number to string for merging\n",
    "    camera_features['Lot_number'] = camera_features['Lot_number'].astype(str)\n",
    "    \n",
    "    # Create binary has_fixed_camera feature\n",
    "    camera_features['has_fixed_camera'] = (camera_features['camera_classification_v2'] == 'FIXED_CAMERA_HIGH').astype(int)\n",
    "    \n",
    "    # Get lot-to-zone mapping\n",
    "    lot_mapping = pd.read_csv('../../data/lot_mapping_enhanced.csv')\n",
    "    lot_mapping['Lot_number'] = lot_mapping['Lot_number'].astype(str)\n",
    "    \n",
    "    # Merge camera with lot mapping to get zones\n",
    "    camera_with_zones = camera_features.merge(\n",
    "        lot_mapping[['Lot_number', 'Zone_Name']],\n",
    "        on='Lot_number',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename Zone_Name to Zone for consistency\n",
    "    camera_with_zones = camera_with_zones.rename(columns={'Zone_Name': 'Zone'})\n",
    "    \n",
    "    # Aggregate to zone level (take max scores per zone)\n",
    "    zone_camera_features = camera_with_zones.groupby('Zone').agg({\n",
    "        'entrance_camera_score': 'max',\n",
    "        'bulk_patrol_score': 'max',\n",
    "        'avg_scans_per_active_hour': 'mean',\n",
    "        'has_fixed_camera': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Merge with occupancy data\n",
    "    occupancy_df = occupancy_df.merge(zone_camera_features, on='Zone', how='left')\n",
    "    \n",
    "    # Fill NaN for zones without camera data\n",
    "    occupancy_df['entrance_camera_score'] = occupancy_df['entrance_camera_score'].fillna(0)\n",
    "    occupancy_df['bulk_patrol_score'] = occupancy_df['bulk_patrol_score'].fillna(0)\n",
    "    occupancy_df['avg_scans_per_active_hour'] = occupancy_df['avg_scans_per_active_hour'].fillna(0)\n",
    "    occupancy_df['has_fixed_camera'] = occupancy_df['has_fixed_camera'].fillna(0).astype(int)\n",
    "    \n",
    "    print(f\"\\n✓ Camera features merged\")\n",
    "    print(f\"  Zones with fixed cameras: {(occupancy_df.groupby('Zone')['has_fixed_camera'].first() == 1).sum()}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Camera classification file not found - skipping camera features\")\n",
    "    occupancy_df['entrance_camera_score'] = 0\n",
    "    occupancy_df['bulk_patrol_score'] = 0\n",
    "    occupancy_df['avg_scans_per_active_hour'] = 0\n",
    "    occupancy_df['has_fixed_camera'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Ground Truth Capacity & Calculate Availability Metrics\n",
    "\n",
    "Use actual max capacity from Transportation Services instead of percentile estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth capacity data from Transportation Services\n",
    "print(\"=\"*70)\n",
    "print(\"LOADING GROUND TRUTH CAPACITY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "zone_capacity = pd.read_csv('../../data/zone_capacity.csv')\n",
    "print(f\"\\nCapacity data loaded for {len(zone_capacity)} zones\")\n",
    "print(f\"\\nTop 10 zones by capacity:\")\n",
    "print(zone_capacity.nlargest(10, 'Max_Capacity'))\n",
    "\n",
    "# Merge capacity data with occupancy - using a more memory-efficient approach\n",
    "# Convert to dictionary for faster lookups\n",
    "capacity_dict = dict(zip(zone_capacity['Zone'], zone_capacity['Max_Capacity']))\n",
    "\n",
    "# Add Max_Capacity column using map (more memory efficient than merge)\n",
    "occupancy_df['Max_Capacity'] = occupancy_df['Zone'].map(capacity_dict)\n",
    "\n",
    "# Check for zones without capacity\n",
    "zones_without_capacity = occupancy_df[occupancy_df['Max_Capacity'].isna()]['Zone'].unique()\n",
    "if len(zones_without_capacity) > 0:\n",
    "    print(f\"\\n⚠️ WARNING: {len(zones_without_capacity)} zones have no capacity data:\")\n",
    "    for zone in zones_without_capacity[:5]:\n",
    "        print(f\"  - {zone}\")\n",
    "    if len(zones_without_capacity) > 5:\n",
    "        print(f\"  ... and {len(zones_without_capacity) - 5} more\")\n",
    "    \n",
    "    # For zones without capacity, fall back to 95th percentile as estimate\n",
    "    print(f\"\\n  Using 95th percentile as fallback estimate for zones without capacity\")\n",
    "    \n",
    "    for zone in zones_without_capacity:\n",
    "        zone_data = occupancy_df[occupancy_df['Zone'] == zone]['occupancy_count']\n",
    "        estimated_capacity = zone_data.quantile(0.95)\n",
    "        occupancy_df.loc[occupancy_df['Zone'] == zone, 'Max_Capacity'] = max(estimated_capacity, 20)\n",
    "\n",
    "# Calculate occupancy ratio using ACTUAL CAPACITY\n",
    "occupancy_df['occupancy_ratio'] = occupancy_df['occupancy_count'] / occupancy_df['Max_Capacity'].replace(0, 1)\n",
    "\n",
    "# Calculate available spaces (NEW - this is what users want to see!)\n",
    "occupancy_df['available_spaces'] = (occupancy_df['Max_Capacity'] - occupancy_df['occupancy_count']).clip(lower=0)\n",
    "\n",
    "# Binary indicators\n",
    "occupancy_df['is_near_full'] = (occupancy_df['occupancy_ratio'] >= 0.85).astype(int)\n",
    "occupancy_df['is_very_full'] = (occupancy_df['occupancy_ratio'] >= 0.95).astype(int)\n",
    "occupancy_df['availability_score'] = (1 - occupancy_df['occupancy_ratio']).clip(0, 1)\n",
    "\n",
    "# Categorize availability\n",
    "def categorize_availability(ratio):\n",
    "    if ratio >= 0.95:\n",
    "        return 'VERY_LOW'  # 95%+ full - very hard to find spot\n",
    "    elif ratio >= 0.80:\n",
    "        return 'LOW'  # 80-95% full - hard to find spot\n",
    "    elif ratio >= 0.60:\n",
    "        return 'MODERATE'  # 60-80% full - moderate chance\n",
    "    elif ratio >= 0.40:\n",
    "        return 'HIGH'  # 40-60% full - good chance\n",
    "    else:\n",
    "        return 'VERY_HIGH'  # <40% full - very easy\n",
    "\n",
    "occupancy_df['availability_category'] = occupancy_df['occupancy_ratio'].apply(categorize_availability)\n",
    "\n",
    "print(f\"\\n✓ Availability metrics calculated using GROUND TRUTH CAPACITY\")\n",
    "print(f\"\\nCapacity statistics:\")\n",
    "print(f\"  Average zone capacity: {occupancy_df.groupby('Zone')['Max_Capacity'].first().mean():.0f} spaces\")\n",
    "print(f\"  Total parking capacity: {occupancy_df.groupby('Zone')['Max_Capacity'].first().sum():.0f} spaces\")\n",
    "print(f\"\\nAvailability distribution:\")\n",
    "print(occupancy_df['availability_category'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"OCCUPANCY SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal zone-interval observations: {len(occupancy_df):,} (15-minute granularity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Camera Classification\n",
    "\n",
    "Add fixed camera features from notebook 16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load camera classification\n",
    "try:\n",
    "    camera_features = pd.read_csv('../../data/processed/fixed_camera_classification.csv')\n",
    "    print(f\"Camera classification loaded: {len(camera_features)} lots\")\n",
    "    \n",
    "    # Convert Lot_number to string for merging\n",
    "    camera_features['Lot_number'] = camera_features['Lot_number'].astype(str)\n",
    "    \n",
    "    # Create binary has_fixed_camera feature from classification\n",
    "    camera_features['has_fixed_camera'] = (camera_features['camera_classification_v2'] == 'FIXED_CAMERA_HIGH').astype(int)\n",
    "    \n",
    "    # Get lot-to-zone mapping from lot_mapping_enhanced.csv (single source of truth)\n",
    "    \"    lot_mapping = pd.read_csv('../../data/lot_mapping_enhanced.csv')\\n\",\n",
    "    lot_mapping['Lot_number'] = lot_mapping['Lot_number'].astype(str)\n",
    "    \n",
    "    # Merge camera with lot mapping to get zones\n",
    "    camera_with_zones = camera_features.merge(\n",
    "        lot_mapping[['Lot_number', 'Zone_Name']],\n",
    "        on='Lot_number',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Rename Zone_Name to Zone for consistency\n",
    "    camera_with_zones = camera_with_zones.rename(columns={'Zone_Name': 'Zone'})\n",
    "    \n",
    "    # Drop rows where Zone is NaN (lots not in mapping)\n",
    "    camera_with_zones = camera_with_zones.dropna(subset=['Zone'])\n",
    "    \n",
    "    print(f\"  Lots matched to zones: {len(camera_with_zones)}\")\n",
    "    \n",
    "    # Aggregate to zone level (take max scores per zone)\n",
    "    zone_camera_features = camera_with_zones.groupby('Zone').agg({\n",
    "        'entrance_camera_score': 'max',\n",
    "        'bulk_patrol_score': 'max',\n",
    "        'avg_scans_per_active_hour': 'mean',\n",
    "        'has_fixed_camera': 'max'\n",
    "    }).reset_index()\n",
    "    \n",
    "    print(f\"  Unique zones with camera data: {len(zone_camera_features)}\")\n",
    "    \n",
    "    # Merge with occupancy data\n",
    "    occupancy_df = occupancy_df.merge(zone_camera_features, on='Zone', how='left')\n",
    "    \n",
    "    # Fill NaN for zones without camera data (use .get() to safely access columns)\n",
    "    for col in ['entrance_camera_score', 'bulk_patrol_score', 'avg_scans_per_active_hour']:\n",
    "        if col in occupancy_df.columns:\n",
    "            occupancy_df[col] = occupancy_df[col].fillna(0)\n",
    "        else:\n",
    "            occupancy_df[col] = 0\n",
    "    \n",
    "    if 'has_fixed_camera' in occupancy_df.columns:\n",
    "        occupancy_df['has_fixed_camera'] = occupancy_df['has_fixed_camera'].fillna(0).astype(int)\n",
    "    else:\n",
    "        occupancy_df['has_fixed_camera'] = 0\n",
    "    \n",
    "    print(f\"\\n✓ Camera features merged\")\n",
    "    print(f\"  Zones with fixed cameras: {(occupancy_df.groupby('Zone')['has_fixed_camera'].first() == 1).sum()}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"⚠️ Camera classification file not found - skipping camera features\")\n",
    "    occupancy_df['entrance_camera_score'] = 0\n",
    "    occupancy_df['bulk_patrol_score'] = 0\n",
    "    occupancy_df['avg_scans_per_active_hour'] = 0\n",
    "    occupancy_df['has_fixed_camera'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Zone Baselines & Availability Metrics\n",
    "\n",
    "Create \"typical full\" baselines using percentiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\"*70)\n",
    "print(\"OCCUPANCY SUMMARY STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTotal zone-hour observations: {len(occupancy_df):,}\")\n",
    "print(f\"\\nOccupancy count distribution:\")\n",
    "print(occupancy_df['occupancy_count'].describe())\n",
    "\n",
    "print(f\"\\n\\nBusiest zones (average occupancy):\")\n",
    "zone_avg = occupancy_df.groupby('Zone')['occupancy_count'].mean().sort_values(ascending=False)\n",
    "print(zone_avg.head(10))\n",
    "\n",
    "print(f\"\\n\\nPeak hours (average occupancy):\")\n",
    "hour_avg = occupancy_df.groupby('hour')['occupancy_count'].mean().sort_values(ascending=False)\n",
    "print(hour_avg.head(10))\n",
    "\n",
    "print(f\"\\n\\nNear-full frequency by zone (% of time zone is 85%+ full):\")\n",
    "near_full_freq = occupancy_df.groupby('Zone')['is_near_full'].mean().sort_values(ascending=False)\n",
    "print((near_full_freq.head(10) * 100).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize occupancy patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. Average occupancy by hour\n",
    "hour_avg.plot(kind='bar', ax=axes[0,0], color='steelblue')\n",
    "axes[0,0].set_title('Average Occupancy by Hour of Day', fontsize=14, fontweight='bold')\n",
    "axes[0,0].set_xlabel('Hour')\n",
    "axes[0,0].set_ylabel('Average Cars Parked')\n",
    "axes[0,0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Average occupancy by day of week\n",
    "day_avg = occupancy_df.groupby('day_of_week')['occupancy_count'].mean()\n",
    "day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[0,1].bar(range(7), day_avg, color='coral')\n",
    "axes[0,1].set_xticks(range(7))\n",
    "axes[0,1].set_xticklabels(day_names)\n",
    "axes[0,1].set_title('Average Occupancy by Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[0,1].set_ylabel('Average Cars Parked')\n",
    "axes[0,1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Top zones by occupancy\n",
    "zone_avg.head(10).plot(kind='barh', ax=axes[1,0], color='teal')\n",
    "axes[1,0].set_title('Top 10 Zones by Average Occupancy', fontsize=14, fontweight='bold')\n",
    "axes[1,0].set_xlabel('Average Cars Parked')\n",
    "axes[1,0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# 4. Game day vs regular day\n",
    "game_comparison = occupancy_df.groupby(['is_game_day', 'hour'])['occupancy_count'].mean().unstack(0)\n",
    "game_comparison.plot(ax=axes[1,1], marker='o')\n",
    "axes[1,1].set_title('Game Day vs Regular Day Occupancy', fontsize=14, fontweight='bold')\n",
    "axes[1,1].set_xlabel('Hour of Day')\n",
    "axes[1,1].set_ylabel('Average Cars Parked')\n",
    "axes[1,1].legend(['Regular Day', 'Game Day'])\n",
    "axes[1,1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../../data/processed/occupancy_patterns.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"Occupancy patterns visualization saved to: data/processed/occupancy_patterns.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Train/Validation/Test Splits\n",
    "\n",
    "Using the same time-based splits as before:\n",
    "- Training: 2020-08 to 2023-12 (~70%)\n",
    "- Validation: 2024-01 to 2024-08 (~15%)\n",
    "- Test: 2024-09 to 2025-11 (~15%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define split dates\n",
    "train_end = pd.Timestamp('2023-12-31')\n",
    "val_end = pd.Timestamp('2024-08-31')\n",
    "\n",
    "# Create splits\n",
    "occupancy_train = occupancy_df[occupancy_df['datetime'] <= train_end].copy()\n",
    "occupancy_val = occupancy_df[(occupancy_df['datetime'] > train_end) & (occupancy_df['datetime'] <= val_end)].copy()\n",
    "occupancy_test = occupancy_df[occupancy_df['datetime'] > val_end].copy()\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"DATA SPLITS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  Records: {len(occupancy_train):,}\")\n",
    "print(f\"  Date range: {occupancy_train['date'].min()} to {occupancy_train['date'].max()}\")\n",
    "print(f\"  Avg occupancy: {occupancy_train['occupancy_count'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nValidation set:\")\n",
    "print(f\"  Records: {len(occupancy_val):,}\")\n",
    "print(f\"  Date range: {occupancy_val['date'].min()} to {occupancy_val['date'].max()}\")\n",
    "print(f\"  Avg occupancy: {occupancy_val['occupancy_count'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  Records: {len(occupancy_test):,}\")\n",
    "print(f\"  Date range: {occupancy_test['date'].min()} to {occupancy_test['date'].max()}\")\n",
    "print(f\"  Avg occupancy: {occupancy_test['occupancy_count'].mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full occupancy data\n",
    "occupancy_df.to_csv('../../data/processed/occupancy_full.csv', index=False)\n",
    "print(f\"Full occupancy data saved: {len(occupancy_df):,} records\")\n",
    "\n",
    "# Save splits\n",
    "occupancy_train.to_csv('../../data/processed/occupancy_train.csv', index=False)\n",
    "occupancy_val.to_csv('../../data/processed/occupancy_val.csv', index=False)\n",
    "occupancy_test.to_csv('../../data/processed/occupancy_test.csv', index=False)\n",
    "\n",
    "print(f\"\\nTrain set saved: {len(occupancy_train):,} records\")\n",
    "print(f\"Validation set saved: {len(occupancy_val):,} records\")\n",
    "print(f\"Test set saved: {len(occupancy_test):,} records\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
