{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lot-Level Occupancy Data Transformation (LPR-Based)\n",
    "\n",
    "This notebook creates **lot-level occupancy estimates** using License Plate Reader (LPR) scan data.\n",
    "\n",
    "**Why LPR-Based?**\n",
    "- AMP session data only covers 46 lots\n",
    "- **LPR data covers 183 lots!** (missing only lots 117, 149, 185)\n",
    "- LPR scans serve as a proxy for parking activity/occupancy\n",
    "- Enables lot-specific predictions for almost every parking lot on campus\n",
    "\n",
    "**Approach:**\n",
    "1. Load LPR data (FY23, FY24, FY25)\n",
    "2. Count hourly LPR scans per lot\n",
    "3. Create lag features (previous hours' scan counts)\n",
    "4. Add temporal, calendar, and weather features\n",
    "5. Create train/val/test splits\n",
    "\n",
    "**Note:** LPR scans don't directly equal occupancy, but they correlate with parking activity. High scans = high turnover/enforcement attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOT-LEVEL OCCUPANCY DATA TRANSFORMATION (LPR-BASED)\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"LOT-LEVEL OCCUPANCY DATA TRANSFORMATION (LPR-BASED)\")\n",
    "print(\"=\"*80)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load LPR Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LPR data from all fiscal years...\n",
      "FY23 LPR scans: 520,850\n",
      "FY24 LPR scans: 612,428\n",
      "FY25 LPR scans: 646,909\n",
      "Total LPR scans: 1,780,187\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading LPR data from all fiscal years...\")\n",
    "\n",
    "# Load LPR data\n",
    "lpr23 = pd.read_excel('../../data/raw/Data_For_Class_Project.xlsx', sheet_name='LPR_Reads_FY23')\n",
    "lpr24 = pd.read_excel('../../data/raw/Data_For_Class_Project.xlsx', sheet_name='LPR_Reads_FY24')\n",
    "lpr25 = pd.read_excel('../../data/raw/Data_For_Class_Project.xlsx', sheet_name='LPR_Reads_FY25')\n",
    "\n",
    "# Combine all years\n",
    "lpr_all = pd.concat([lpr23, lpr24, lpr25], ignore_index=True)\n",
    "\n",
    "print(f\"FY23 LPR scans: {len(lpr23):,}\")\n",
    "print(f\"FY24 LPR scans: {len(lpr24):,}\")\n",
    "print(f\"FY25 LPR scans: {len(lpr25):,}\")\n",
    "print(f\"Total LPR scans: {len(lpr_all):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After parsing:\n",
      "  LPR scans with lot numbers: 1,780,187\n",
      "  Unique lots: 185\n",
      "  Date range: 2022-07-01 05:24:26 to 2025-06-30 21:58:49\n"
     ]
    }
   ],
   "source": [
    "# Parse lot numbers from LOT column\n",
    "lpr_all['lot_number'] = lpr_all['LOT'].str.extract(r'LOT\\s+(\\d+)')[0].astype(float)\n",
    "\n",
    "# Parse datetime\n",
    "lpr_all['datetime'] = pd.to_datetime(lpr_all['Date_Time'])\n",
    "\n",
    "# Remove rows without lot number\n",
    "lpr_all = lpr_all[lpr_all['lot_number'].notna()].copy()\n",
    "lpr_all['lot_number'] = lpr_all['lot_number'].astype(int)\n",
    "\n",
    "print(f\"\\nAfter parsing:\")\n",
    "print(f\"  LPR scans with lot numbers: {len(lpr_all):,}\")\n",
    "print(f\"  Unique lots: {lpr_all['lot_number'].nunique()}\")\n",
    "print(f\"  Date range: {lpr_all['datetime'].min()} to {lpr_all['datetime'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Lot Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lots in mapping: 186\n",
      "\n",
      "Lots with LPR data: 185\n",
      "Lots missing from LPR: {185, 117, 149}\n"
     ]
    }
   ],
   "source": [
    "# Load lot mapping to get zone info and capacity\n",
    "lot_mapping = pd.read_csv('../../data/lot_mapping_enhanced_with_coords.csv')\n",
    "\n",
    "print(f\"Total lots in mapping: {len(lot_mapping)}\")\n",
    "\n",
    "# Create lot_number -> zone mapping\n",
    "lot_to_zone = lot_mapping.set_index('Lot_number')['Zone_Name'].to_dict()\n",
    "lot_to_capacity = lot_mapping.set_index('Lot_number')['capacity'].to_dict()\n",
    "\n",
    "# Check LPR coverage\n",
    "lpr_lots = set(lpr_all['lot_number'].unique())\n",
    "mapping_lots = set(lot_mapping['Lot_number'].unique())\n",
    "missing_from_lpr = mapping_lots - lpr_lots\n",
    "\n",
    "print(f\"\\nLots with LPR data: {len(lpr_lots)}\")\n",
    "print(f\"Lots missing from LPR: {missing_from_lpr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Create Hourly LPR Scan Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating hourly LPR scan counts per lot...\n",
      "Hourly LPR data: 76,550 lot-hour records\n",
      "Average scans per hour: 23.26\n",
      "Max scans in one hour: 666\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating hourly LPR scan counts per lot...\")\n",
    "\n",
    "# Extract date and hour\n",
    "lpr_all['date'] = lpr_all['datetime'].dt.date\n",
    "lpr_all['hour'] = lpr_all['datetime'].dt.hour\n",
    "\n",
    "# Count scans per lot-date-hour\n",
    "lpr_hourly = lpr_all.groupby(['lot_number', 'date', 'hour']).size().reset_index(name='lpr_scans')\n",
    "\n",
    "print(f\"Hourly LPR data: {len(lpr_hourly):,} lot-hour records\")\n",
    "print(f\"Average scans per hour: {lpr_hourly['lpr_scans'].mean():.2f}\")\n",
    "print(f\"Max scans in one hour: {lpr_hourly['lpr_scans'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Complete Time Grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating complete lot-hour grid...\n",
      "Lot-hour grid created: 4,866,240 combinations\n",
      "  185 lots\n",
      "  1096 days\n",
      "  24 hours per day\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating complete lot-hour grid...\")\n",
    "\n",
    "# Date range\n",
    "start_date = lpr_all['datetime'].min().date()\n",
    "end_date = lpr_all['datetime'].max().date()\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Create grid for all lots with LPR data\n",
    "lots_with_lpr = sorted(lpr_lots)\n",
    "hours = range(24)\n",
    "\n",
    "lot_hour_grid = pd.MultiIndex.from_product(\n",
    "    [lots_with_lpr, date_range, hours],\n",
    "    names=['lot_number', 'date', 'hour']\n",
    ").to_frame(index=False)\n",
    "\n",
    "lot_hour_grid['date'] = lot_hour_grid['date'].dt.date\n",
    "\n",
    "print(f\"Lot-hour grid created: {len(lot_hour_grid):,} combinations\")\n",
    "print(f\"  {len(lots_with_lpr)} lots\")\n",
    "print(f\"  {len(date_range)} days\")\n",
    "print(f\"  24 hours per day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged LPR scans into grid\n",
      "Total records: 4,866,240\n",
      "Hours with scans: 76,550\n",
      "Hours without scans: 4,789,690\n"
     ]
    }
   ],
   "source": [
    "# Merge LPR scan counts\n",
    "occupancy_df = lot_hour_grid.merge(\n",
    "    lpr_hourly,\n",
    "    on=['lot_number', 'date', 'hour'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Fill missing scans with 0\n",
    "occupancy_df['lpr_scans'] = occupancy_df['lpr_scans'].fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nMerged LPR scans into grid\")\n",
    "print(f\"Total records: {len(occupancy_df):,}\")\n",
    "print(f\"Hours with scans: {(occupancy_df['lpr_scans'] > 0).sum():,}\")\n",
    "print(f\"Hours without scans: {(occupancy_df['lpr_scans'] == 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Add Lot Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding lot metadata (zone, capacity)...\n",
      "  Estimating capacity for 26 lots...\n",
      "  Lot metadata added\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding lot metadata (zone, capacity)...\")\n",
    "\n",
    "occupancy_df['Zone'] = occupancy_df['lot_number'].map(lot_to_zone)\n",
    "occupancy_df['capacity'] = occupancy_df['lot_number'].map(lot_to_capacity)\n",
    "\n",
    "# Fill missing capacity with estimated values\n",
    "lots_without_cap = occupancy_df[occupancy_df['capacity'].isna()]['lot_number'].unique()\n",
    "if len(lots_without_cap) > 0:\n",
    "    print(f\"  Estimating capacity for {len(lots_without_cap)} lots...\")\n",
    "    for lot_num in lots_without_cap:\n",
    "        # Use 95th percentile of LPR scans as capacity estimate\n",
    "        lot_scans = occupancy_df[occupancy_df['lot_number'] == lot_num]['lpr_scans']\n",
    "        estimated_cap = max(lot_scans.quantile(0.95) * 2, 10)  # Assume scans are ~50% of capacity\n",
    "        occupancy_df.loc[occupancy_df['lot_number'] == lot_num, 'capacity'] = estimated_cap\n",
    "\n",
    "print(\"  Lot metadata added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Add Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding temporal features...\n",
      "  Temporal features added\n"
     ]
    }
   ],
   "source": [
    "print(\"Adding temporal features...\")\n",
    "\n",
    "occupancy_df['date'] = pd.to_datetime(occupancy_df['date'])\n",
    "occupancy_df['datetime'] = occupancy_df['date'] + pd.to_timedelta(occupancy_df['hour'], unit='h')\n",
    "\n",
    "occupancy_df['year'] = occupancy_df['datetime'].dt.year\n",
    "occupancy_df['month'] = occupancy_df['datetime'].dt.month\n",
    "occupancy_df['day'] = occupancy_df['datetime'].dt.day\n",
    "occupancy_df['day_of_week'] = occupancy_df['datetime'].dt.dayofweek\n",
    "occupancy_df['is_weekend'] = (occupancy_df['day_of_week'] >= 5).astype(int)\n",
    "\n",
    "def categorize_time_of_day(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'Late Night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'Morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'Afternoon'\n",
    "    elif 18 <= hour < 22:\n",
    "        return 'Evening'\n",
    "    else:\n",
    "        return 'Night'\n",
    "\n",
    "occupancy_df['time_of_day'] = occupancy_df['hour'].apply(categorize_time_of_day)\n",
    "print(\"  Temporal features added\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Lag Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating lag features (previous hours' scans)...\n",
      "  Created lag feature: 1h ago\n",
      "  Created lag feature: 2h ago\n",
      "  Created lag feature: 3h ago\n",
      "  Created lag feature: 24h ago\n",
      "  Created lag feature: 168h ago\n",
      "  Lag features created\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating lag features (previous hours' scans)...\")\n",
    "\n",
    "# Sort by lot and datetime\n",
    "occupancy_df = occupancy_df.sort_values(['lot_number', 'datetime']).reset_index(drop=True)\n",
    "\n",
    "# Create lag features for each lot\n",
    "lag_hours = [1, 2, 3, 24, 168]  # 1h, 2h, 3h, 1 day, 1 week ago\n",
    "\n",
    "for lag in lag_hours:\n",
    "    occupancy_df[f'lpr_scans_lag_{lag}h'] = occupancy_df.groupby('lot_number')['lpr_scans'].shift(lag)\n",
    "    print(f\"  Created lag feature: {lag}h ago\")\n",
    "\n",
    "# Fill missing lag values with 0\n",
    "for lag in lag_hours:\n",
    "    occupancy_df[f'lpr_scans_lag_{lag}h'] = occupancy_df[f'lpr_scans_lag_{lag}h'].fillna(0).astype(int)\n",
    "\n",
    "print(\"  Lag features created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Merge Calendar Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging calendar events...\n",
      "  Calendar events merged\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging calendar events...\")\n",
    "\n",
    "games = pd.read_csv('../../data/football_games.csv')\n",
    "calendar = pd.read_csv('../../data/academic_calendar.csv')\n",
    "\n",
    "games['Date'] = pd.to_datetime(games['Date'])\n",
    "calendar['Start_Date'] = pd.to_datetime(calendar['Start_Date']).dt.normalize()\n",
    "calendar['End_Date'] = pd.to_datetime(calendar['End_Date']).dt.normalize()\n",
    "\n",
    "game_dates = games['Date'].dt.normalize()\n",
    "occupancy_df['is_game_day'] = occupancy_df['date'].isin(game_dates).astype(int)\n",
    "\n",
    "# Calendar events\n",
    "for event_type in ['Dead_Week', 'Finals_Week', 'Spring_Break', 'Thanksgiving_Break', 'Winter_Break']:\n",
    "    event_periods = calendar[calendar['Event_Type'] == event_type]\n",
    "    occupancy_df[f'is_{event_type.lower()}'] = 0\n",
    "    \n",
    "    for _, period in event_periods.iterrows():\n",
    "        mask = (occupancy_df['date'] >= period['Start_Date']) & \\\n",
    "               (occupancy_df['date'] <= period['End_Date'])\n",
    "        occupancy_df.loc[mask, f'is_{event_type.lower()}'] = 1\n",
    "\n",
    "occupancy_df['is_any_break'] = (\n",
    "    occupancy_df['is_spring_break'] |\n",
    "    occupancy_df['is_thanksgiving_break'] |\n",
    "    occupancy_df['is_winter_break']\n",
    ").astype(int)\n",
    "\n",
    "print(\"  Calendar events merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Merge Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging weather data...\n",
      "  Weather data merged\n"
     ]
    }
   ],
   "source": [
    "print(\"Merging weather data...\")\n",
    "\n",
    "weather = pd.read_csv('../../data/weather_pullman_2020_2025.csv')\n",
    "weather['date'] = pd.to_datetime(weather['date']).dt.normalize()\n",
    "\n",
    "occupancy_df = occupancy_df.merge(weather, left_on='date', right_on='date', how='left')\n",
    "print(\"  Weather data merged\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Create Train/Validation/Test Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train/validation/test splits...\n",
      "Training: 3,516,665 records (2022-07-01 00:00:00 to 2024-08-31 00:00:00)\n",
      "Validation: 1,212,120 records (2024-08-31 00:00:00 to 2025-05-31 00:00:00)\n",
      "Test: 137,455 records (2025-05-31 00:00:00 to 2025-06-30 00:00:00)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating train/validation/test splits...\")\n",
    "\n",
    "train_end = pd.Timestamp('2024-08-31')\n",
    "val_end = pd.Timestamp('2025-05-31')\n",
    "\n",
    "occupancy_train = occupancy_df[occupancy_df['datetime'] <= train_end].copy()\n",
    "occupancy_val = occupancy_df[(occupancy_df['datetime'] > train_end) & (occupancy_df['datetime'] <= val_end)].copy()\n",
    "occupancy_test = occupancy_df[occupancy_df['datetime'] > val_end].copy()\n",
    "\n",
    "print(f\"Training: {len(occupancy_train):,} records ({occupancy_train['date'].min()} to {occupancy_train['date'].max()})\")\n",
    "print(f\"Validation: {len(occupancy_val):,} records ({occupancy_val['date'].min()} to {occupancy_val['date'].max()})\")\n",
    "print(f\"Test: {len(occupancy_test):,} records ({occupancy_test['date'].min()} to {occupancy_test['date'].max()})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Save Lot-Level Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving lot-level occupancy data (LPR-based)...\n",
      "Full lot-level data saved: 4,866,240 records\n",
      "Train set saved: 3,516,665 records\n",
      "Validation set saved: 1,212,120 records\n",
      "Test set saved: 137,455 records\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving lot-level occupancy data (LPR-based)...\")\n",
    "\n",
    "occupancy_df.to_csv('../../data/processed/occupancy_lot_level_lpr_full.csv', index=False)\n",
    "occupancy_train.to_csv('../../data/processed/occupancy_lot_level_lpr_train.csv', index=False)\n",
    "occupancy_val.to_csv('../../data/processed/occupancy_lot_level_lpr_val.csv', index=False)\n",
    "occupancy_test.to_csv('../../data/processed/occupancy_lot_level_lpr_test.csv', index=False)\n",
    "\n",
    "print(f\"Full lot-level data saved: {len(occupancy_df):,} records\")\n",
    "print(f\"Train set saved: {len(occupancy_train):,} records\")\n",
    "print(f\"Validation set saved: {len(occupancy_val):,} records\")\n",
    "print(f\"Test set saved: {len(occupancy_test):,} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOT-LEVEL DATA SUMMARY (LPR-BASED)\n",
      "================================================================================\n",
      "Lots with data: 185\n",
      "Average LPR scans per hour: 0.37\n",
      "Date range: 2022-07-01 00:00:00 to 2025-06-30 00:00:00\n",
      "\n",
      "Top 10 lots by LPR scan activity:\n",
      "lot_number\n",
      "150    6.818849\n",
      "71     4.589682\n",
      "9      4.294936\n",
      "26     3.942404\n",
      "124    3.629866\n",
      "146    3.361694\n",
      "104    2.461603\n",
      "1      2.123936\n",
      "120    1.877775\n",
      "47     1.646442\n",
      "Name: lpr_scans, dtype: float64\n",
      "\n",
      "================================================================================\n",
      "DONE!\n",
      "================================================================================\n",
      "\n",
      "Next steps:\n",
      "1. Train lot-level model using LPR scans as target\n",
      "2. Use lag features for prediction (previous hours' activity)\n",
      "3. Compare with zone-level model performance\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"LOT-LEVEL DATA SUMMARY (LPR-BASED)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Lots with data: {occupancy_df['lot_number'].nunique()}\")\n",
    "print(f\"Average LPR scans per hour: {occupancy_df['lpr_scans'].mean():.2f}\")\n",
    "print(f\"Date range: {occupancy_df['date'].min()} to {occupancy_df['date'].max()}\")\n",
    "print(\"\\nTop 10 lots by LPR scan activity:\")\n",
    "print(occupancy_df.groupby('lot_number')['lpr_scans'].mean().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DONE!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nNext steps:\")\n",
    "print(\"1. Train lot-level model using LPR scans as target\")\n",
    "print(\"2. Use lag features for prediction (previous hours' activity)\")\n",
    "print(\"3. Compare with zone-level model performance\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
