{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extend Enforcement Data to October 2025\n",
    "This notebook extends enforcement data through October 2025 using historical LPR patterns.\n",
    "## Data Situation\n",
    "- **LPR data**: 2022-07-01 to 2025-06-30 ✅\n",
    "- **Ticket data**: 2018-07-02 to 2025-10-30 ✅\n",
    "- **AMP data**: 2020-08-10 to 2025-11-02 ✅\n",
    "## Strategy\n",
    "For July-October 2025:\n",
    "1. Use **actual ticket and AMP data** (ground truth enforcement)\n",
    "2. **Estimate LPR** from historical 2022-2024 patterns\n",
    "3. Calculate enforcement metrics as usual\n",
    "4. Add `lpr_estimated` flag for model to learn from\n",
    "## Why This Works\n",
    "- LPR shows **WHEN/WHERE** enforcement patrols (timing patterns are stable)\n",
    "- Tickets show **HOW AGGRESSIVE** enforcement is (may increase over time)\n",
    "- Historical LPR timing + Actual 2025 tickets = Best estimate of current enforcement risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries loaded\n"
     ]
    }
   ],
   "source": "import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (14, 6)\nprint(\"Libraries loaded\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded:\n",
      "  AMP sessions: 1,702,867\n",
      "  LPR reads: 1,780,391\n",
      "  Tickets: 192,709\n",
      "\n",
      "Date ranges:\n",
      "  AMP: 2020-08-10 to 2025-11-02\n",
      "  LPR: 2022-07-01 to 2025-06-30\n",
      "  Tickets: 2018-07-02 to 2025-10-30\n",
      "\n",
      "⚠️  LPR gap: 2025-06-30 to 2025-10-30\n",
      "   Need to estimate 122 days of LPR data\n"
     ]
    }
   ],
   "source": "# Load all data sources\namp = pd.read_csv('../../data/processed/amp_preprocessed.csv', parse_dates=['Start_Date', 'End_Date'])\nlpr = pd.read_csv('../../data/processed/lpr_preprocessed.csv', parse_dates=['Date_Time'])\ntickets = pd.read_csv('../../data/processed/tickets_enriched.csv', parse_dates=['Issue_DateTime'])\nlot_mapping = pd.read_csv('../../data/lot_mapping_enhanced.csv')\namp_aliases = pd.read_csv('../../data/amp_zone_aliases.csv')\nprint(\"Data loaded:\")\nprint(f\"  AMP sessions: {len(amp):,}\")\nprint(f\"  LPR reads: {len(lpr):,}\")\nprint(f\"  Tickets: {len(tickets):,}\")\nprint(f\"\\nDate ranges:\")\nprint(f\"  AMP: {amp['Start_Date'].min().date()} to {amp['End_Date'].max().date()}\")\nprint(f\"  LPR: {lpr['Date_Time'].min().date()} to {lpr['Date_Time'].max().date()}\")\nprint(f\"  Tickets: {tickets['Issue_DateTime'].min().date()} to {tickets['Issue_DateTime'].max().date()}\")\nlpr_end_date = lpr['Date_Time'].max().date()\ntickets_end_date = tickets['Issue_DateTime'].max().date()\nprint(f\"\\n⚠️  LPR gap: {lpr_end_date} to {tickets_end_date}\")\ngap_days = (tickets_end_date - lpr_end_date).days\nprint(f\"   Need to estimate {gap_days} days of LPR data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate Historical LPR Patterns\n",
    "Calculate average LPR scans per zone-day_of_week-hour-month from 2022-2024 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historical LPR data: 2022-07-01 to 2025-06-30\n",
      "Total LPR records: 1,780,391\n",
      "\n",
      "Historical LPR (2022-2024): 1,476,561 records\n",
      "\n",
      "Calculating LPR patterns (zone-day_of_week-hour-month)...\n",
      "  Primary patterns: 11,270 combinations\n",
      "\n",
      "Calculating fallback patterns (zone-hour-month)...\n",
      "  Fallback patterns: 3,485 combinations\n",
      "\n",
      "Sample patterns (Paid zone):\n",
      "                                  num_dates  avg_scans\n",
      "Zone_Name day_of_week hour month                      \n",
      "Paid      0           0    1              1        1.0\n",
      "                           3              2        1.0\n",
      "                           4              3        1.3\n",
      "                           5              1        2.0\n",
      "                           8              2        1.0\n",
      "                           9              2        2.0\n",
      "                           10             1        1.0\n",
      "                           11             2        1.0\n",
      "                           12             3        1.3\n",
      "                      1    1              1        1.0\n"
     ]
    }
   ],
   "source": "# LPR data is already preprocessed - each row is one scan\n# Add date column for counting unique dates\nlpr['date'] = lpr['Date_Time'].dt.date\nprint(f\"Historical LPR data: {lpr['Date_Time'].min().date()} to {lpr['Date_Time'].max().date()}\")\nprint(f\"Total LPR records: {len(lpr):,}\")\n# Filter to historical data only (2022-2024)\nlpr_historical = lpr[lpr['Date_Time'] < '2025-01-01'].copy()\nprint(f\"\\nHistorical LPR (2022-2024): {len(lpr_historical):,} records\")\n# Calculate patterns: count scans per zone-day_of_week-hour-month\nprint(\"\\nCalculating LPR patterns (zone-day_of_week-hour-month)...\")\nlpr_patterns = lpr_historical.groupby(['Zone_Name', 'day_of_week', 'hour', 'month']).size().to_frame('total_scans')\n# Also count unique dates to get average\ndate_counts = lpr_historical.groupby(['Zone_Name', 'day_of_week', 'hour', 'month'])['date'].nunique().to_frame('num_dates')\nlpr_patterns = lpr_patterns.join(date_counts)\nlpr_patterns['avg_scans'] = (lpr_patterns['total_scans'] / lpr_patterns['num_dates']).round(1)\nprint(f\"  Primary patterns: {len(lpr_patterns):,} combinations\")\n# Fallback patterns (no day_of_week, for sparse data)\nprint(\"\\nCalculating fallback patterns (zone-hour-month)...\")\nlpr_patterns_fallback = lpr_historical.groupby(['Zone_Name', 'hour', 'month']).size().to_frame('total_scans')\ndate_counts_fallback = lpr_historical.groupby(['Zone_Name', 'hour', 'month'])['date'].nunique().to_frame('num_dates')\nlpr_patterns_fallback = lpr_patterns_fallback.join(date_counts_fallback)\nlpr_patterns_fallback['avg_scans'] = (lpr_patterns_fallback['total_scans'] / lpr_patterns_fallback['num_dates']).round(1)\nprint(f\"  Fallback patterns: {len(lpr_patterns_fallback):,} combinations\")\n# Show sample patterns\nprint(\"\\nSample patterns (Paid zone):\")\npaid_patterns = lpr_patterns[lpr_patterns.index.get_level_values('Zone_Name') == 'Paid'].head(10)\nif len(paid_patterns) > 0:\n    print(paid_patterns[['num_dates', 'avg_scans']])\nelse:\n    print(\"  No Paid zone patterns found - trying other zones...\")\n    # Show any patterns\n    print(lpr_patterns.head(10)[['num_dates', 'avg_scans']])"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Extended Time Grid (Through October 2025)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extended time range: 2022-07-01 to 2025-10-30\n",
      "Processing 25 zones: ['Apartments', 'Authorized Vehicles Only', 'Blue 1', 'Buisness Parking', 'Crimson 2', 'Disability', 'Gray 1', 'Green 1', 'Green 2', 'Green 3', 'Green 4', 'Green 5', 'Grey 2', 'Orange 4', 'Paid', 'Red 1', 'Red 4', 'Red 5', 'Visitor', 'Yellow 1', 'Yellow 2', 'Yellow 3', 'Yellow 4', 'Yellow 5', 'Yellow5']\n",
      "\n",
      "Time grid created: 29,232 hourly intervals\n",
      "  With actual LPR data: 26,304\n",
      "  Using estimated LPR: 2,928\n",
      "\n",
      "Breakdown by year:\n",
      "  2022: 4,416 intervals (4,416 actual LPR, 0 estimated)\n",
      "  2023: 8,760 intervals (8,760 actual LPR, 0 estimated)\n",
      "  2024: 8,784 intervals (8,784 actual LPR, 0 estimated)\n",
      "  2025: 7,272 intervals (4,344 actual LPR, 2,928 estimated)\n"
     ]
    }
   ],
   "source": "# Extended time range\nstart_date = pd.Timestamp('2022-07-01').date()\nend_date = tickets_end_date\nprint(f\"Extended time range: {start_date} to {end_date}\")\n# Get ticket zones\nticket_zones = tickets['Zone_Name'].dropna().unique()\nprint(f\"Processing {len(ticket_zones)} zones: {sorted(ticket_zones)}\")\n# Create time grid\ndate_range = pd.date_range(start=start_date, end=end_date, freq='D')\nhours = range(24)\ntime_grid = pd.MultiIndex.from_product(\n    [date_range, hours],\n    names=['date', 'hour']\n).to_frame(index=False)\ntime_grid['datetime'] = time_grid['date'] + pd.to_timedelta(time_grid['hour'], unit='h')\ntime_grid['day_of_week'] = time_grid['datetime'].dt.dayofweek\ntime_grid['month'] = time_grid['datetime'].dt.month\ntime_grid['has_lpr_data'] = time_grid['date'].dt.date <= lpr_end_date\nprint(f\"\\nTime grid created: {len(time_grid):,} hourly intervals\")\nprint(f\"  With actual LPR data: {time_grid['has_lpr_data'].sum():,}\")\nprint(f\"  Using estimated LPR: {(~time_grid['has_lpr_data']).sum():,}\")\nprint(f\"\\nBreakdown by year:\")\ntime_grid['year'] = time_grid['datetime'].dt.year\nfor year in sorted(time_grid['year'].unique()):\n    year_data = time_grid[time_grid['year'] == year]\n    actual_lpr = year_data['has_lpr_data'].sum()\n    estimated_lpr = (~year_data['has_lpr_data']).sum()\n    print(f\"  {year}: {len(year_data):,} intervals ({actual_lpr:,} actual LPR, {estimated_lpr:,} estimated)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Map AMP Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AMP zones mapped: 1,698,169 / 1,702,867\n"
     ]
    }
   ],
   "source": "# Map AMP zones to standard names\namp = amp.merge(\n    amp_aliases[['AMP_Zone_Name', 'Standard_Zone_Name']],\n    left_on='Zone',\n    right_on='AMP_Zone_Name',\n    how='left'\n)\nprint(f\"AMP zones mapped: {amp['Standard_Zone_Name'].notna().sum():,} / {len(amp):,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Process Enforcement Data with LPR Estimation\n",
    "This is the main processing loop. For each zone and hour:\n",
    "- Count actual AMP sessions and tickets\n",
    "- Use actual LPR if available, otherwise estimate from historical patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing enforcement data...\n",
      "This will take 5-10 minutes.\n",
      "\n",
      "  5/25: Yellow 4\n",
      "  10/25: Green 1\n",
      "  15/25: Yellow5\n",
      "  20/25: Apartments\n",
      "  25/25: Visitor\n",
      "\n",
      " Enforcement data created: 730,800 records\n",
      "  Date range: 2022-07-01 00:00:00 to 2025-10-30 00:00:00\n"
     ]
    }
   ],
   "source": "print(\"Processing enforcement data...\")\nprint(\"This will take 5-10 minutes.\\n\")\nenforcement_list = []\nfor i, zone in enumerate(ticket_zones, 1):\n    if i % 5 == 0:\n        print(f\"  {i}/{len(ticket_zones)}: {zone}\")\n    # Filter data for this zone\n    zone_amp = amp[amp['Zone'].str.contains(zone, case=False, na=False)].copy()\n    zone_lpr = lpr[lpr['Zone_Name'].str.contains(zone, case=False, na=False)].copy()\n    zone_tickets = tickets[tickets['Zone_Name'] == zone].copy()\n    for _, row in time_grid.iterrows():\n        hour_start = row['datetime']\n        hour_end = hour_start + pd.Timedelta(hours=1)\n        day_of_week = row['day_of_week']\n        hour = row['hour']\n        month = row['month']\n        has_lpr = row['has_lpr_data']\n        # Count AMP sessions active during this hour\n        amp_count = len(zone_amp[\n            (zone_amp['Start_Date'] < hour_end) &\n            (zone_amp['End_Date'] > hour_start)\n        ])\n        # Count tickets issued during this hour\n        ticket_count = len(zone_tickets[\n            (zone_tickets['Issue_DateTime'] >= hour_start) &\n            (zone_tickets['Issue_DateTime'] < hour_end)\n        ])\n        # LPR count: actual or estimated\n        if has_lpr:\n            # Use actual LPR data\n            lpr_count = len(zone_lpr[\n                (zone_lpr['Date_Time'] >= hour_start) &\n                (zone_lpr['Date_Time'] < hour_end)\n            ])\n            lpr_estimated = False\n        else:\n            # Estimate from historical pattern\n            pattern_key = (zone, day_of_week, hour, month)\n            if pattern_key in lpr_patterns.index:\n                lpr_count = int(lpr_patterns.loc[pattern_key, 'avg_scans'])\n            else:\n                # Try fallback pattern (no day_of_week)\n                fallback_key = (zone, hour, month)\n                if fallback_key in lpr_patterns_fallback.index:\n                    lpr_count = int(lpr_patterns_fallback.loc[fallback_key, 'avg_scans'])\n                else:\n                    # Use zone-hour average across all months\n                    zone_hour_data = lpr_historical[\n                        (lpr_historical['Zone_Name'] == zone) &\n                        (lpr_historical['hour'] == hour)\n                    ]\n                    if len(zone_hour_data) > 0:\n                        lpr_count = int(zone_hour_data.groupby('date').size().mean())\n                    else:\n                        lpr_count = 0\n            lpr_estimated = True\n        # Calculate enforcement metrics\n        unpaid_estimate = max(0, lpr_count - amp_count)\n        enforcement_rate = ticket_count / unpaid_estimate if unpaid_estimate > 0 else 0\n        enforcement_list.append({\n            'Zone': zone,\n            'date': row['date'],\n            'hour': hour,\n            'datetime': hour_start,\n            'lpr_scans': lpr_count,\n            'amp_sessions': amp_count,\n            'tickets_issued': ticket_count,\n            'unpaid_estimate': unpaid_estimate,\n            'enforcement_rate': enforcement_rate,\n            'lpr_estimated': lpr_estimated\n        })\nenforcement_df = pd.DataFrame(enforcement_list)\nprint(f\"\\n Enforcement data created: {len(enforcement_df):,} records\")\nprint(f\"  Date range: {enforcement_df['date'].min()} to {enforcement_df['date'].max()}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Verify Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATA QUALITY CHECK\n",
      "================================================================================\n",
      "\n",
      "LPR data breakdown:\n",
      "  Actual LPR: 657,600 records (90.0%)\n",
      "  Estimated LPR: 73,200 records (10.0%)\n",
      "\n",
      "Ticket counts (all actual data):\n",
      "  With actual LPR: 46,822\n",
      "  With estimated LPR: 6,671\n",
      "  Total tickets: 53,493\n",
      "\n",
      "2025 breakdown:\n",
      "  Total 2025 records: 181,800\n",
      "  Jan-June 2025 (actual LPR): 108,600 records, 8,257 tickets\n",
      "  July-Oct 2025 (estimated LPR): 73,200 records, 6,671 tickets\n",
      "\n",
      "CUE Garage / Paid zone check:\n",
      "  Total 2025 records: 7,272\n",
      "  Estimated LPR records: 2,928\n",
      "  Tickets with estimated LPR: 3,520\n"
     ]
    }
   ],
   "source": "print(\"=\"*80)\nprint(\"DATA QUALITY CHECK\")\nprint(\"=\"*80)\nprint(f\"\\nLPR data breakdown:\")\nprint(f\"  Actual LPR: {(~enforcement_df['lpr_estimated']).sum():,} records ({(~enforcement_df['lpr_estimated']).sum()/len(enforcement_df)*100:.1f}%)\")\nprint(f\"  Estimated LPR: {enforcement_df['lpr_estimated'].sum():,} records ({enforcement_df['lpr_estimated'].sum()/len(enforcement_df)*100:.1f}%)\")\nprint(f\"\\nTicket counts (all actual data):\")\nprint(f\"  With actual LPR: {enforcement_df[~enforcement_df['lpr_estimated']]['tickets_issued'].sum():,}\")\nprint(f\"  With estimated LPR: {enforcement_df[enforcement_df['lpr_estimated']]['tickets_issued'].sum():,}\")\nprint(f\"  Total tickets: {enforcement_df['tickets_issued'].sum():,}\")\nprint(f\"\\n2025 breakdown:\")\ndata_2025 = enforcement_df[enforcement_df['datetime'].dt.year == 2025]\nprint(f\"  Total 2025 records: {len(data_2025):,}\")\njan_june = data_2025[~data_2025['lpr_estimated']]\njuly_oct = data_2025[data_2025['lpr_estimated']]\nprint(f\"  Jan-June 2025 (actual LPR): {len(jan_june):,} records, {jan_june['tickets_issued'].sum():,} tickets\")\nprint(f\"  July-Oct 2025 (estimated LPR): {len(july_oct):,} records, {july_oct['tickets_issued'].sum():,} tickets\")\n# Check specific zones\nprint(f\"\\nCUE Garage / Paid zone check:\")\ncue_data = enforcement_df[enforcement_df['Zone'] == 'Paid']\nif len(cue_data) > 0:\n    cue_2025 = cue_data[cue_data['datetime'].dt.year == 2025]\n    cue_estimated = cue_2025[cue_2025['lpr_estimated']]\n    print(f\"  Total 2025 records: {len(cue_2025):,}\")\n    print(f\"  Estimated LPR records: {len(cue_estimated):,}\")\n    print(f\"  Tickets with estimated LPR: {cue_estimated['tickets_issued'].sum():,}\")\nelse:\n    print(f\"  No 'Paid' zone found - check zone names\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Add Temporal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal features added\n",
      "\n",
      "Year distribution:\n",
      "  2022: 110,400 records, 7,675 tickets (0 with estimated LPR)\n",
      "  2023: 219,000 records, 15,733 tickets (0 with estimated LPR)\n",
      "  2024: 219,600 records, 15,157 tickets (0 with estimated LPR)\n",
      "  2025: 181,800 records, 14,928 tickets (73,200 with estimated LPR)\n"
     ]
    }
   ],
   "source": "# Add temporal features\nenforcement_df['year'] = enforcement_df['datetime'].dt.year\nenforcement_df['month'] = enforcement_df['datetime'].dt.month\nenforcement_df['day_of_week'] = enforcement_df['datetime'].dt.dayofweek\nenforcement_df['is_weekend'] = (enforcement_df['day_of_week'] >= 5).astype(int)\ndef categorize_time_of_day(hour):\n    if 0 <= hour < 6:\n        return 'Late Night'\n    elif 6 <= hour < 12:\n        return 'Morning'\n    elif 12 <= hour < 18:\n        return 'Afternoon'\n    elif 18 <= hour < 22:\n        return 'Evening'\n    else:\n        return 'Night'\nenforcement_df['time_of_day'] = enforcement_df['hour'].apply(categorize_time_of_day)\ntime_of_day_map = {'Afternoon': 0, 'Evening': 1, 'Late Night': 2, 'Morning': 3, 'Night': 4}\nenforcement_df['time_of_day_code'] = enforcement_df['time_of_day'].map(time_of_day_map)\nprint(\"Temporal features added\")\nprint(f\"\\nYear distribution:\")\nfor year in sorted(enforcement_df['year'].unique()):\n    year_data = enforcement_df[enforcement_df['year'] == year]\n    tickets = year_data['tickets_issued'].sum()\n    estimated = year_data['lpr_estimated'].sum()\n    print(f\"  {year}: {len(year_data):,} records, {tickets:,} tickets ({estimated:,} with estimated LPR)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Merge Calendar Events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calendar events merged\n",
      "  Game day hours: 13,800\n",
      "  Finals week hours: 18,000\n",
      "  Break hours: 70,800\n"
     ]
    }
   ],
   "source": "# Load calendar data\ngames = pd.read_csv('../../data/football_games.csv', parse_dates=['Date'])\ncalendar = pd.read_csv('../../data/academic_calendar.csv', parse_dates=['Start_Date', 'End_Date'])\n# Convert date for merging\nenforcement_df['date'] = pd.to_datetime(enforcement_df['date'])\n# Mark game days\ngame_dates = games['Date'].dt.date.unique()\nenforcement_df['is_game_day'] = enforcement_df['date'].dt.date.isin(game_dates).astype(int)\n# Mark calendar events\nfor event_type in ['Dead_Week', 'Finals_Week', 'Spring_Break', 'Thanksgiving_Break', 'Winter_Break']:\n    event_periods = calendar[calendar['Event_Type'] == event_type]\n    enforcement_df[f'is_{event_type.lower()}'] = 0\n    for _, period in event_periods.iterrows():\n        mask = (enforcement_df['date'] >= period['Start_Date']) & \\\n               (enforcement_df['date'] <= period['End_Date'])\n        enforcement_df.loc[mask, f'is_{event_type.lower()}'] = 1\n# Combined break indicator\nenforcement_df['is_any_break'] = (\n    enforcement_df['is_spring_break'] | \n    enforcement_df['is_thanksgiving_break'] | \n    enforcement_df['is_winter_break']\n).astype(int)\nprint(\"Calendar events merged\")\nprint(f\"  Game day hours: {enforcement_df['is_game_day'].sum():,}\")\nprint(f\"  Finals week hours: {enforcement_df['is_finals_week'].sum():,}\")\nprint(f\"  Break hours: {enforcement_df['is_any_break'].sum():,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Merge Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weather data merged\n",
      "  Weather features added: ['temperature_f', 'precipitation_inches', 'snowfall_inches', 'snow_depth_inches', 'wind_mph', 'weather_code', 'weather_category', 'is_rainy', 'is_snowy', 'is_cold', 'is_hot', 'is_windy', 'is_severe']\n"
     ]
    }
   ],
   "source": "# Load weather data\nweather = pd.read_csv('../../data/weather_pullman_hourly_2020_2025.csv', parse_dates=['datetime'])\nweather['date'] = pd.to_datetime(weather['date']).dt.date\nweather['hour'] = weather['hour'].astype(int)\n# Merge on date + hour\nenforcement_df['date_for_merge'] = enforcement_df['date'].dt.date\nenforcement_df = enforcement_df.merge(\n    weather, \n    left_on=['date_for_merge', 'hour'], \n    right_on=['date', 'hour'], \n    how='left'\n)\n# Clean up merge columns\nenforcement_df = enforcement_df.drop(columns=['date_for_merge', 'date_y', 'datetime_y'], errors='ignore')\nenforcement_df = enforcement_df.rename(columns={'date_x': 'date', 'datetime_x': 'datetime'})\nprint(\"Weather data merged\")\nweather_cols = [col for col in weather.columns if col not in ['date', 'hour', 'datetime', 'year']]\nprint(f\"  Weather features added: {weather_cols}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Extended Enforcement Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "EXTENDED ENFORCEMENT DATA SAVED\n",
      "================================================================================\n",
      "\n",
      "File: ../../data/processed/enforcement_full_extended.csv\n",
      "Records: 730,800\n",
      "Date range: 2022-07-01 00:00:00 to 2025-10-30 00:00:00\n",
      "Columns: 37\n",
      "\n",
      "Data summary:\n",
      "  Total tickets: 53,493\n",
      "  Actual LPR: 657,600 records\n",
      "  Estimated LPR: 73,200 records\n",
      "\n",
      "October 2025 data:\n",
      "  Records: 18,000\n",
      "  Tickets: 1,742\n",
      "  All using estimated LPR: True\n"
     ]
    }
   ],
   "source": "# Save to file\noutput_path = '../../data/processed/enforcement_full_extended.csv'\nenforcement_df.to_csv(output_path, index=False)\nprint(\"=\"*80)\nprint(\"EXTENDED ENFORCEMENT DATA SAVED\")\nprint(\"=\"*80)\nprint(f\"\\nFile: {output_path}\")\nprint(f\"Records: {len(enforcement_df):,}\")\nprint(f\"Date range: {enforcement_df['date'].min()} to {enforcement_df['date'].max()}\")\nprint(f\"Columns: {len(enforcement_df.columns)}\")\nprint(f\"\\nData summary:\")\nprint(f\"  Total tickets: {enforcement_df['tickets_issued'].sum():,}\")\nprint(f\"  Actual LPR: {(~enforcement_df['lpr_estimated']).sum():,} records\")\nprint(f\"  Estimated LPR: {enforcement_df['lpr_estimated'].sum():,} records\")\nprint(f\"\\nOctober 2025 data:\")\noct_2025 = enforcement_df[\n    (enforcement_df['datetime'] >= '2025-10-01') & \n    (enforcement_df['datetime'] < '2025-11-01')\n]\nprint(f\"  Records: {len(oct_2025):,}\")\nprint(f\"  Tickets: {oct_2025['tickets_issued'].sum():,}\")\nprint(f\"  All using estimated LPR: {oct_2025['lpr_estimated'].all()}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}